{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><strong><em>Let's import all the required libraries and modules</em></strong></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rn\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization\n",
    "from keras.layers import Activation, Dropout, ZeroPadding3D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling3D, Conv3D, MaxPooling2D\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "rn.seed(30)\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('./Project_data/val.csv').readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><span style=\"color: #ff0000;\">Batch Size</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are building using a test generator class for easier understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, width=120, height=120, frames=30, channel=3, \n",
    "                 crop = True, normalize = False, affine = False, flip = False, edge = False  ):\n",
    "        self.width = width   # X dimension of the image\n",
    "        self.height = height # Y dimesnion of the image\n",
    "        self.frames = frames # length/depth of the video frames\n",
    "        self.channel = channel # number of channels in images 3 for color(RGB) and 1 for Gray  \n",
    "        self.affine = affine # augment data with affine transform of the image\n",
    "        self.flip = flip\n",
    "        self.normalize =  normalize\n",
    "        self.edge = edge # edge detection\n",
    "        self.crop = crop\n",
    "\n",
    "    # Helper function to generate a random affine transform on the image\n",
    "    def __get_random_affine(self): # private method\n",
    "        dx, dy = np.random.randint(-1.7, 1.8, 2)\n",
    "        M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "        return M\n",
    "\n",
    "    # Helper function to initialize all the batch image data and labels\n",
    "    def __init_batch_data(self, batch_size): # private method\n",
    "        batch_data = np.zeros((batch_size, self.frames, self.width, self.height, self.channel)) \n",
    "        batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    def __load_batch_images(self, source_path, folder_list, batch_num, batch_size, t): # private method\n",
    "    \n",
    "        batch_data,batch_labels = self.__init_batch_data(batch_size)\n",
    "        # We will also build a agumented batch data\n",
    "        if self.affine:\n",
    "            batch_data_aug,batch_labels_aug = self.__init_batch_data(batch_size)\n",
    "        if self.flip:\n",
    "            batch_data_flip,batch_labels_flip = self.__init_batch_data(batch_size)\n",
    "\n",
    "        #create a list of image numbers you want to use for a particular video\n",
    "        img_idx = [x for x in range(0, self.frames)] \n",
    "\n",
    "        for folder in range(batch_size): # iterate over the batch_size\n",
    "            # read all the images in the folder\n",
    "            imgs = sorted(os.listdir(source_path+'/'+ t[folder + (batch_num*batch_size)].split(';')[0])) \n",
    "            # Generate a random affine to be used in image transformation for buidling agumented data set\n",
    "            M = self.__get_random_affine()\n",
    "            \n",
    "            #  Iterate over the frames/images of a folder to read them in\n",
    "            for idx, item in enumerate(img_idx): \n",
    "                image = cv2.imread(source_path+'/'+ t[folder + (batch_num*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                #and the conv3D will throw error if the inputs in a batch have different shapes  \n",
    "                if self.crop:\n",
    "                    image = self.__crop(image)\n",
    "                # If normalize is set normalize the image else use the raw image.\n",
    "                if self.normalize:\n",
    "                    resized = self.__normalize(self.__resize(image))\n",
    "                else:\n",
    "                    resized = self.__resize(image)\n",
    "                # If the input is edge detected image then use the sobelx, sobely and laplacian as 3 channel of the edge detected image\n",
    "                if self.edge:\n",
    "                    resized = self.__edge(resized)\n",
    "                \n",
    "                batch_data[folder,idx] = resized\n",
    "                if self.affine:\n",
    "                    batch_data_aug[folder,idx] = self.__affine(resized, M)   \n",
    "                if self.flip:\n",
    "                    batch_data_flip[folder,idx] = self.__flip(resized)   \n",
    "\n",
    "            batch_labels[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            if self.affine:\n",
    "                batch_labels_aug[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            if self.flip:\n",
    "                if int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==0:\n",
    "                    batch_labels_flip[folder, 1] = 1\n",
    "                elif int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==1:\n",
    "                    batch_labels_flip[folder, 0] = 1\n",
    "                else:\n",
    "                    batch_labels_flip[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "        \n",
    "        if self.affine:\n",
    "            batch_data = np.append(batch_data, batch_data_aug, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "        if self.flip:\n",
    "            batch_data = np.append(batch_data, batch_data_flip, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_flip, axis = 0) \n",
    "\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    def generator(self, source_path, folder_list, batch_size): # public method\n",
    "        print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "            for batch in range(num_batches): # we iterate over the number of batches\n",
    "                # you yield the batch_data and the batch_labels, remember what does yield do\n",
    "                yield self.__load_batch_images(source_path, folder_list, batch, batch_size, t) \n",
    "            \n",
    "            # write the code for the remaining data points which are left after full batches\n",
    "            if (len(folder_list) != batch_size*num_batches):\n",
    "                batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "                yield self.__load_batch_images(source_path, folder_list, num_batches, batch_size, t)\n",
    "                \n",
    "## Helper functions for image processing\n",
    "\n",
    "    #Affine transform on the image\n",
    "    def __affine(self, image, M):\n",
    "        return cv2.warpAffine(image, M, (image.shape[0], image.shape[1]))\n",
    "\n",
    "    # Flipping the image\n",
    "    def __flip(self, image):\n",
    "        return np.flip(image,1)\n",
    "    \n",
    "    # Helper function to normalise the data\n",
    "    def __normalize(self, image):\n",
    "        return image/127.5-1\n",
    "    \n",
    "    # Resizing the image\n",
    "    def __resize(self, image):\n",
    "        return cv2.resize(image, (self.width,self.height), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    # Cropoing the image\n",
    "    def __crop(self, image):\n",
    "        if image.shape[0] != image.shape[1]:\n",
    "            return image[0:120, 20:140]\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "    # Edge detection\n",
    "    def __edge(self, image):\n",
    "        edge = np.zeros((image.shape[0], image.shape[1], image.shape[2]))\n",
    "        edge[:,:,0] = cv2.Laplacian(cv2.GaussianBlur(image[:,:,0],(3,3),0),cv2.CV_64F)\n",
    "        edge[:,:,1] = cv2.Laplacian(cv2.GaussianBlur(image[:,:,1],(3,3),0),cv2.CV_64F)\n",
    "        edge[:,:,2] = cv2.Laplacian(cv2.GaussianBlur(image[:,:,2],(3,3),0),cv2.CV_64F)\n",
    "        return edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, num_epochs, model, train_generator, val_generator, optimiser=None):\n",
    "\n",
    "    curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "    num_train_sequences = len(train_doc)\n",
    "    print('# training sequences =', num_train_sequences)\n",
    "    num_val_sequences = len(val_doc)\n",
    "    print('# validation sequences =', num_val_sequences)\n",
    "    print('# batch size =', batch_size)    \n",
    "    print('# epochs =', num_epochs)\n",
    "\n",
    "    #write your optimizer\n",
    "    if optimiser == None:\n",
    "        optimiser = Adam() \n",
    "    model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    print (model.summary())\n",
    "    \n",
    "    model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "    if not os.path.exists(model_name):\n",
    "        os.mkdir(model_name)\n",
    "            \n",
    "    filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath, \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=False, \n",
    "                                 save_weights_only=False, \n",
    "                                 mode='auto', \n",
    "                                 period=1)\n",
    "    LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "    callbacks_list = [checkpoint, LR]\n",
    "\n",
    "    if (num_train_sequences%batch_size) == 0:\n",
    "        steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "    else:\n",
    "        steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "    if (num_val_sequences%batch_size) == 0:\n",
    "        validation_steps = int(num_val_sequences/batch_size)\n",
    "    else:\n",
    "        validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "    model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                callbacks=callbacks_list, validation_data=val_generator, \n",
    "                validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "    \n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGenerator(object):\n",
    "    \n",
    "    @classmethod\n",
    "    def c3d1(cls, input_shape, nb_classes):\n",
    "        \"\"\"\n",
    "        Build a 3D convolutional network, based loosely on C3D.\n",
    "            https://arxiv.org/pdf/1412.0767.pdf\n",
    "        \"\"\"\n",
    "        # Model.\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(\n",
    "            8, (3,3,3), activation='relu', input_shape=input_shape\n",
    "        ))\n",
    "        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "        model.add(Conv3D(16, (3,3,3), activation='relu'))\n",
    "        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "        model.add(Conv3D(32, (3,3,3), activation='relu'))\n",
    "        model.add(Conv3D(32, (3,3,3), activation='relu'))\n",
    "        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "        model.add(Conv3D(64, (2,2,2), activation='relu'))\n",
    "        model.add(Conv3D(64, (2,2,2), activation='relu'))\n",
    "        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(256))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    @classmethod\n",
    "    def c3d2(cls, input_shape, nb_classes):\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, kernel_size=(3, 3, 3), input_shape=input_shape, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv3D(16, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    ## CNN(Conv2D) + RNN(LSTM)\n",
    "    def lrcn(cls, input_shape, nb_classes):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2),\n",
    "            activation='relu', padding='same'), input_shape=input_shape))\n",
    "        model.add(TimeDistributed(Conv2D(32, (3,3),\n",
    "            kernel_initializer=\"he_normal\", activation='relu')))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(64, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(Conv2D(64, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(128, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(Conv2D(128, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Conv2D(256, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(Conv2D(256, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "        \n",
    "        model.add(TimeDistributed(Conv2D(512, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(Conv2D(512, (3,3),\n",
    "            padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(LSTM(256, return_sequences=False, dropout=0.5))\n",
    "        model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './Project_data/train'\n",
    "val_path = './Project_data/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1a : Resize to 120*120,  Raw image input, No cropping, No normalisation, No agumentation, No flipped images, No edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# batch size = 20\n",
      "# epochs = 20\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_7 (Conv3D)            (None, 28, 118, 118, 8)   656       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 28, 59, 59, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 26, 57, 57, 16)    3472      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 26, 28, 28, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 24, 26, 26, 32)    13856     \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 22, 24, 24, 32)    27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 22, 12, 12, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 21, 11, 11, 64)    16448     \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 20, 10, 10, 64)    32832     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 20, 5, 5, 64)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               16384512  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 16,612,069\n",
      "Trainable params: 16,612,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - 46s 1s/step - loss: 12.8845 - categorical_accuracy: 0.1994 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1512_18_35.152588/model-00001-12.93778-0.19608-12.57211-0.22000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 8s 233ms/step - loss: 13.4317 - categorical_accuracy: 0.1667 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1512_18_35.152588/model-00002-13.43175-0.16667-12.57211-0.22000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 8s 223ms/step - loss: 13.1157 - categorical_accuracy: 0.1863 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1512_18_35.152588/model-00003-13.11570-0.18627-12.57211-0.22000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 8s 243ms/step - loss: 13.4317 - categorical_accuracy: 0.1667 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1512_18_35.152588/model-00004-13.43175-0.16667-12.57211-0.22000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 8s 225ms/step - loss: 12.4836 - categorical_accuracy: 0.2255 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1512_18_35.152588/model-00005-12.48362-0.22549-12.57211-0.22000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 9s 264ms/step - loss: 13.2737 - categorical_accuracy: 0.1765 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1512_18_35.152588/model-00006-13.27373-0.17647-12.57211-0.22000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 8s 222ms/step - loss: 12.1676 - categorical_accuracy: 0.2451 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1512_18_35.152588/model-00007-12.16758-0.24510-12.57211-0.22000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 8s 223ms/step - loss: 13.1157 - categorical_accuracy: 0.1863 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1512_18_35.152588/model-00008-13.11570-0.18627-12.57211-0.22000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 8s 225ms/step - loss: 14.0638 - categorical_accuracy: 0.1275 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1512_18_35.152588/model-00009-14.06383-0.12745-12.57211-0.22000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 9s 259ms/step - loss: 13.1157 - categorical_accuracy: 0.1863 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1512_18_35.152588/model-00010-13.11570-0.18627-12.57211-0.22000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 9s 259ms/step - loss: 13.1157 - categorical_accuracy: 0.1863 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1512_18_35.152588/model-00011-13.11570-0.18627-12.57211-0.22000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 9s 259ms/step - loss: 12.0096 - categorical_accuracy: 0.2549 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1512_18_35.152588/model-00012-12.00956-0.25490-12.57211-0.22000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 13.2737 - categorical_accuracy: 0.1765 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1512_18_35.152588/model-00013-13.27373-0.17647-12.57211-0.22000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 9s 260ms/step - loss: 11.6935 - categorical_accuracy: 0.2745 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1512_18_35.152588/model-00014-11.69352-0.27451-12.57211-0.22000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 9s 269ms/step - loss: 12.4836 - categorical_accuracy: 0.2255 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1512_18_35.152588/model-00015-12.48362-0.22549-12.57211-0.22000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 9s 266ms/step - loss: 13.2737 - categorical_accuracy: 0.1765 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1512_18_35.152588/model-00016-13.27373-0.17647-12.57211-0.22000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 9s 262ms/step - loss: 12.6416 - categorical_accuracy: 0.2157 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1512_18_35.152588/model-00017-12.64164-0.21569-12.57211-0.22000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 9s 261ms/step - loss: 12.9577 - categorical_accuracy: 0.1961 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1512_18_35.152588/model-00018-12.95768-0.19608-12.57211-0.22000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 9s 264ms/step - loss: 13.1157 - categorical_accuracy: 0.1863 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1512_18_35.152588/model-00019-13.11570-0.18627-12.57211-0.22000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 9s 258ms/step - loss: 12.6416 - categorical_accuracy: 0.2157 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1512_18_35.152588/model-00020-12.64164-0.21569-12.57211-0.22000.h5\n"
     ]
    }
   ],
   "source": [
    "train_gen = DataGenerator()\n",
    "val_gen = DataGenerator()\n",
    "model_gen = ModelGenerator()\n",
    "\n",
    "input_shape = (30,120,120, 3)\n",
    "num_classes = 5\n",
    "\n",
    "model = model_gen.c3d1(input_shape, num_classes)\n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "\n",
    "train_generator = train_gen.generator(train_path, train_doc, batch_size)\n",
    "val_generator = val_gen.generator(val_path, val_doc, batch_size)\n",
    "train(batch_size, num_epochs, model, train_generator, val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical accuracy after 20 epochs  = 0.22\n",
    "- training sequences = 663\n",
    "- validation sequences = 100\n",
    "- batch size = 20\n",
    "- epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Resize to 120*120,  agumentation, flipped images, normalisation, cropping, edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# batch size = 20\n",
      "# epochs = 20\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_31 (Conv3D)           (None, 28, 118, 118, 8)   656       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling (None, 28, 59, 59, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_32 (Conv3D)           (None, 26, 57, 57, 16)    3472      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_22 (MaxPooling (None, 26, 28, 28, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_33 (Conv3D)           (None, 24, 26, 26, 32)    13856     \n",
      "_________________________________________________________________\n",
      "conv3d_34 (Conv3D)           (None, 22, 24, 24, 32)    27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_23 (MaxPooling (None, 22, 12, 12, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_35 (Conv3D)           (None, 21, 11, 11, 64)    16448     \n",
      "_________________________________________________________________\n",
      "conv3d_36 (Conv3D)           (None, 20, 10, 10, 64)    32832     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 20, 5, 5, 64)      0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               16384512  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 16,612,069\n",
      "Trainable params: 16,612,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  ./Project_data/val ; batch size =Source path =  ./Project_data/train  20\n",
      "; batch size =Epoch 1/20\n",
      " 20\n",
      "34/34 [==============================] - 110s 3s/step - loss: 1.6460 - categorical_accuracy: 0.2762 - val_loss: 12.5721 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1513_04_26.043035/model-00001-1.65851-0.26647-12.57211-0.22000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 16s 481ms/step - loss: 1.6470 - categorical_accuracy: 0.2059 - val_loss: 11.9899 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1513_04_26.043035/model-00002-1.64696-0.20588-11.98994-0.24000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 18s 535ms/step - loss: 1.6099 - categorical_accuracy: 0.2190 - val_loss: 12.4057 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1513_04_26.043035/model-00003-1.60992-0.21895-12.40569-0.23000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 18s 522ms/step - loss: 1.6118 - categorical_accuracy: 0.1993 - val_loss: 9.0483 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1513_04_26.043035/model-00004-1.61184-0.19935-9.04829-0.23000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 19s 557ms/step - loss: 1.6194 - categorical_accuracy: 0.1765 - val_loss: 8.9237 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1513_04_26.043035/model-00005-1.61936-0.17647-8.92372-0.23000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 18s 534ms/step - loss: 1.6278 - categorical_accuracy: 0.1732 - val_loss: 7.9388 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1513_04_26.043035/model-00006-1.62780-0.17320-7.93876-0.19000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 18s 526ms/step - loss: 1.6168 - categorical_accuracy: 0.2386 - val_loss: 8.0823 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1513_04_26.043035/model-00007-1.61679-0.23856-8.08235-0.24000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 18s 530ms/step - loss: 1.6253 - categorical_accuracy: 0.1634 - val_loss: 8.3190 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1513_04_26.043035/model-00008-1.62525-0.16340-8.31896-0.24000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 18s 523ms/step - loss: 1.6246 - categorical_accuracy: 0.1275 - val_loss: 8.6091 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1513_04_26.043035/model-00009-1.62457-0.12745-8.60915-0.24000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 18s 517ms/step - loss: 1.6107 - categorical_accuracy: 0.1797 - val_loss: 8.1332 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1513_04_26.043035/model-00010-1.61073-0.17974-8.13318-0.26000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 19s 554ms/step - loss: 1.6201 - categorical_accuracy: 0.1765 - val_loss: 8.4334 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1513_04_26.043035/model-00011-1.62008-0.17647-8.43343-0.28000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 18s 533ms/step - loss: 1.6161 - categorical_accuracy: 0.1536 - val_loss: 8.7885 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1513_04_26.043035/model-00012-1.61606-0.15359-8.78851-0.25000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 18s 530ms/step - loss: 1.6039 - categorical_accuracy: 0.2745 - val_loss: 8.7759 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1513_04_26.043035/model-00013-1.60387-0.27451-8.77594-0.25000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 17s 514ms/step - loss: 1.6149 - categorical_accuracy: 0.2059 - val_loss: 8.8739 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1513_04_26.043035/model-00014-1.61488-0.20588-8.87395-0.25000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 18s 535ms/step - loss: 1.6145 - categorical_accuracy: 0.1569 - val_loss: 8.8369 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1513_04_26.043035/model-00015-1.61446-0.15686-8.83691-0.25000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 18s 543ms/step - loss: 1.6125 - categorical_accuracy: 0.1830 - val_loss: 8.7591 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1513_04_26.043035/model-00016-1.61245-0.18301-8.75915-0.25000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 18s 543ms/step - loss: 1.6090 - categorical_accuracy: 0.2157 - val_loss: 8.7360 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1513_04_26.043035/model-00017-1.60898-0.21569-8.73601-0.25000.h5\n",
      "Epoch 18/20\n",
      "13/34 [==========>...................] - ETA: 9s - loss: 1.6108 - categorical_accuracy: 0.1966"
     ]
    }
   ],
   "source": [
    "train_gen = DataGenerator(affine=True, flip=True, normalize=True, crop=True, edge=True)\n",
    "val_gen = DataGenerator()\n",
    "model_gen = ModelGenerator()\n",
    "\n",
    "input_shape = (30,120,120, 3)\n",
    "num_classes = 5\n",
    "\n",
    "model = model_gen.c3d1(input_shape, num_classes)\n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "\n",
    "train_generator = train_gen.generator(train_path, train_doc, batch_size)\n",
    "val_generator = val_gen.generator(val_path, val_doc, batch_size)\n",
    "train(batch_size, num_epochs, model, train_generator, val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 3\n",
    "<ul>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\">Resize to 120*120</span></h4>\n",
    "</li>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\">agumentation</span></h4>\n",
    "</li>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\">flipped images</span></h4>\n",
    "</li>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\"> No normalisation</span></h4>\n",
    "</li>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\"> No cropping</span></h4>\n",
    "</li>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\">No edge detection</span></h4>\n",
    "<br /><br /></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# batch size = 10\n",
      "# epochs = 20\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 30, 60, 60, 32)    4736      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 58, 58, 32)    9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 29, 29, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 29, 29, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 30, 29, 29, 64)    36928     \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 30, 14, 14, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 30, 14, 14, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 30, 14, 14, 128)   147584    \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 30, 7, 7, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 30, 7, 7, 256)     295168    \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 30, 7, 7, 256)     590080    \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 30, 3, 3, 256)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 30, 3, 3, 512)     1180160   \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 30, 3, 3, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 30, 1, 1, 512)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 5,504,805\n",
      "Trainable params: 5,504,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  ./Project_data/val ; batch size = 10\n",
      "Source path =  ./Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 100s 1s/step - loss: 1.7727 - categorical_accuracy: 0.1901 - val_loss: 1.6361 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1610_25_33.308826/model-00001-1.77033-0.19206-1.63612-0.16000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 29s 433ms/step - loss: 1.7337 - categorical_accuracy: 0.2106 - val_loss: 1.6277 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1610_25_33.308826/model-00002-1.73372-0.21061-1.62775-0.18000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 29s 435ms/step - loss: 1.7393 - categorical_accuracy: 0.1675 - val_loss: 1.6737 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1610_25_33.308826/model-00003-1.73926-0.16750-1.67366-0.22000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 30s 444ms/step - loss: 1.6905 - categorical_accuracy: 0.2007 - val_loss: 1.6422 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1610_25_33.308826/model-00004-1.69055-0.20066-1.64219-0.23000.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 29s 438ms/step - loss: 1.6793 - categorical_accuracy: 0.2255 - val_loss: 1.7454 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1610_25_33.308826/model-00005-1.67933-0.22554-1.74538-0.23000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 30s 446ms/step - loss: 1.7111 - categorical_accuracy: 0.1907 - val_loss: 1.7146 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1610_25_33.308826/model-00006-1.71115-0.19071-1.71456-0.25000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 30s 454ms/step - loss: 1.7200 - categorical_accuracy: 0.1907 - val_loss: 1.6523 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1610_25_33.308826/model-00007-1.71995-0.19071-1.65234-0.27000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 30s 455ms/step - loss: 1.6652 - categorical_accuracy: 0.2139 - val_loss: 1.6398 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1610_25_33.308826/model-00008-1.66523-0.21393-1.63982-0.23000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 29s 437ms/step - loss: 1.7011 - categorical_accuracy: 0.2139 - val_loss: 1.6752 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1610_25_33.308826/model-00009-1.70107-0.21393-1.67520-0.23000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 30s 441ms/step - loss: 1.7048 - categorical_accuracy: 0.1891 - val_loss: 1.6711 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1610_25_33.308826/model-00010-1.70478-0.18905-1.67112-0.23000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 30s 441ms/step - loss: 1.6723 - categorical_accuracy: 0.1924 - val_loss: 1.6676 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1610_25_33.308826/model-00011-1.67231-0.19237-1.66758-0.22000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 30s 443ms/step - loss: 1.6874 - categorical_accuracy: 0.2189 - val_loss: 1.6629 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1610_25_33.308826/model-00012-1.68740-0.21891-1.66287-0.22000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 30s 440ms/step - loss: 1.6799 - categorical_accuracy: 0.2156 - val_loss: 1.6537 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1610_25_33.308826/model-00013-1.67989-0.21559-1.65371-0.23000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 30s 446ms/step - loss: 1.6883 - categorical_accuracy: 0.2139 - val_loss: 1.6543 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1610_25_33.308826/model-00014-1.68827-0.21393-1.65426-0.25000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 29s 437ms/step - loss: 1.6928 - categorical_accuracy: 0.2222 - val_loss: 1.6601 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1610_25_33.308826/model-00015-1.69277-0.22222-1.66007-0.24000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 30s 449ms/step - loss: 1.7135 - categorical_accuracy: 0.1857 - val_loss: 1.6598 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1610_25_33.308826/model-00016-1.71352-0.18574-1.65982-0.23000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 30s 446ms/step - loss: 1.6662 - categorical_accuracy: 0.2322 - val_loss: 1.6588 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1610_25_33.308826/model-00017-1.66619-0.23217-1.65883-0.23000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 30s 442ms/step - loss: 1.6735 - categorical_accuracy: 0.2040 - val_loss: 1.6576 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1610_25_33.308826/model-00018-1.67348-0.20398-1.65757-0.25000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 29s 435ms/step - loss: 1.6606 - categorical_accuracy: 0.2239 - val_loss: 1.6613 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1610_25_33.308826/model-00019-1.66058-0.22388-1.66125-0.25000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 29s 434ms/step - loss: 1.6739 - categorical_accuracy: 0.1924 - val_loss: 1.6606 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1610_25_33.308826/model-00020-1.67395-0.19237-1.66057-0.25000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n"
     ]
    }
   ],
   "source": [
    "train_gen = DataGenerator(affine=True, flip=True)\n",
    "val_gen = DataGenerator()\n",
    "model_gen = ModelGenerator()\n",
    "\n",
    "input_shape = (30,120,120, 3)\n",
    "num_classes = 5\n",
    "\n",
    "model = model_gen.lrcn(input_shape, num_classes)\n",
    "\n",
    "batch_size = 10\n",
    "#num_epochs = 20\n",
    "\n",
    "train_generator = train_gen.generator(train_path, train_doc, batch_size)\n",
    "val_generator = val_gen.generator(val_path, val_doc, batch_size)\n",
    "train(batch_size, num_epochs, model, train_generator, val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorical accuracy of 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 4\n",
    "<ul>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\">Resize to 120*120</span></h4>\n",
    "</li>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\">agumentation</span></h4>\n",
    "</li>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\">flipped images</span></h4>\n",
    "</li>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\"> No normalisation</span></h4>\n",
    "</li>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\"> No cropping</span></h4>\n",
    "</li>\n",
    "<li>\n",
    "<h4 id=\"Model-3c-:-Resize-to-120*120,--agumentation,-flipped-images,-No-normalisation,-No-cropping,-No-edge-detection\"><span style=\"color: #339966;\">No edge detection</span></h4>\n",
    "<br /><br /></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# batch size = 10\n",
      "# epochs = 20\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 120, 120, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 30, 120, 120, 16)  6928      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 10, 40, 40, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 40, 40, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 10, 40, 40, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 40, 40, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 10, 40, 40, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10, 40, 40, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 4, 14, 14, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 14, 14, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 4, 14, 14, 32)     27680     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 14, 14, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 4, 14, 14, 32)     27680     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4, 14, 14, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 2, 5, 5, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 5, 5, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 929,461\n",
      "Trainable params: 928,437\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path =  ./Project_data/val ; batch size = 10\n",
      "Source path =  ./Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 194s 3s/step - loss: 2.0256 - categorical_accuracy: 0.3116 - val_loss: 4.8650 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1607_40_59.356783/model-00001-2.02933-0.31021-4.86504-0.22000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 69s 1s/step - loss: 1.6737 - categorical_accuracy: 0.3566 - val_loss: 0.9031 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1607_40_59.356783/model-00002-1.67369-0.35655-0.90315-0.66000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 69s 1s/step - loss: 1.5264 - categorical_accuracy: 0.4461 - val_loss: 2.0214 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1607_40_59.356783/model-00003-1.52638-0.44610-2.02137-0.48000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 69s 1s/step - loss: 1.2512 - categorical_accuracy: 0.5158 - val_loss: 1.3334 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1607_40_59.356783/model-00004-1.25121-0.51575-1.33344-0.53000.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 69s 1s/step - loss: 1.1698 - categorical_accuracy: 0.5423 - val_loss: 0.8651 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1607_40_59.356783/model-00005-1.16977-0.54229-0.86510-0.66000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 69s 1s/step - loss: 1.1572 - categorical_accuracy: 0.5373 - val_loss: 0.7961 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1607_40_59.356783/model-00006-1.15719-0.53731-0.79605-0.66000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 69s 1s/step - loss: 1.1360 - categorical_accuracy: 0.5556 - val_loss: 0.7239 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1607_40_59.356783/model-00007-1.13595-0.55556-0.72390-0.77000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 69s 1s/step - loss: 1.0043 - categorical_accuracy: 0.6070 - val_loss: 0.6537 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1607_40_59.356783/model-00008-1.00426-0.60697-0.65371-0.79000.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 69s 1s/step - loss: 1.0305 - categorical_accuracy: 0.5771 - val_loss: 0.8173 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1607_40_59.356783/model-00009-1.03049-0.57711-0.81729-0.70000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 69s 1s/step - loss: 0.9642 - categorical_accuracy: 0.6252 - val_loss: 0.6518 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1607_40_59.356783/model-00010-0.96422-0.62521-0.65180-0.74000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.9351 - categorical_accuracy: 0.6318 - val_loss: 0.6318 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1607_40_59.356783/model-00011-0.93509-0.63184-0.63180-0.78000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.9344 - categorical_accuracy: 0.6335 - val_loss: 2.3175 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1607_40_59.356783/model-00012-0.93440-0.63350-2.31754-0.44000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 68s 1s/step - loss: 1.2908 - categorical_accuracy: 0.4959 - val_loss: 2.7281 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1607_40_59.356783/model-00013-1.29076-0.49585-2.72814-0.38000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 68s 1s/step - loss: 1.2615 - categorical_accuracy: 0.4975 - val_loss: 0.9468 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1607_40_59.356783/model-00014-1.26154-0.49751-0.94679-0.60000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 68s 1s/step - loss: 1.0230 - categorical_accuracy: 0.5738 - val_loss: 0.8247 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1607_40_59.356783/model-00015-1.02304-0.57380-0.82469-0.68000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 68s 1s/step - loss: 1.0088 - categorical_accuracy: 0.5871 - val_loss: 0.7538 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1607_40_59.356783/model-00016-1.00880-0.58706-0.75383-0.70000.h5\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 68s 1s/step - loss: 1.0124 - categorical_accuracy: 0.6070 - val_loss: 0.7287 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1607_40_59.356783/model-00017-1.01241-0.60697-0.72867-0.71000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.9617 - categorical_accuracy: 0.5937 - val_loss: 0.7536 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1607_40_59.356783/model-00018-0.96165-0.59370-0.75363-0.69000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.9989 - categorical_accuracy: 0.5821 - val_loss: 0.6648 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1607_40_59.356783/model-00019-0.99887-0.58209-0.66484-0.80000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.9417 - categorical_accuracy: 0.6103 - val_loss: 0.6666 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1607_40_59.356783/model-00020-0.94170-0.61028-0.66658-0.79000.h5\n"
     ]
    }
   ],
   "source": [
    "train_gen = DataGenerator(affine=True, flip=True)\n",
    "val_gen = DataGenerator()\n",
    "model_gen = ModelGenerator()\n",
    "\n",
    "input_shape = (30,120,120, 3)\n",
    "num_classes = 5\n",
    "\n",
    "model = model_gen.c3d2(input_shape, num_classes)\n",
    "\n",
    "batch_size = 10\n",
    "#num_epochs = 20\n",
    "\n",
    "train_generator = train_gen.generator(train_path, train_doc, batch_size)\n",
    "val_generator = val_gen.generator(val_path, val_doc, batch_size)\n",
    "train(batch_size, num_epochs, model, train_generator, val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Categorical Validation accuracy: <span style=\"color: #339966;\">79%</span></h2>\n",
    "<ul>\n",
    "<li>Total params: <strong><span style=\"color: #339966;\">929,461</span></strong></li>\n",
    "<li>Trainable params: <strong><span style=\"color: #339966;\">928,437</span></strong><br /><br /></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to improve this model with edge detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# batch size = 20\n",
      "# epochs = 20\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 120, 120, 16)  1312      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 30, 120, 120, 16)  6928      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 120, 120, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 10, 40, 40, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 40, 40, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 10, 40, 40, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 40, 40, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 10, 40, 40, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10, 40, 40, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 4, 14, 14, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 14, 14, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 4, 14, 14, 32)     27680     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 14, 14, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 4, 14, 14, 32)     27680     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4, 14, 14, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 2, 5, 5, 32)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 5, 5, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 929,461\n",
      "Trainable params: 928,437\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n",
      "Source path = Source path =  ./Project_data/train ; batch size =Epoch 1/20\n",
      "  20\n",
      "./Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 153s 5s/step - loss: 2.0520 - categorical_accuracy: 0.3152 - val_loss: 10.4725 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1609_52_37.134220/model-00001-2.06718-0.31473-10.47249-0.21000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.8482 - categorical_accuracy: 0.3758 - val_loss: 11.0638 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1609_52_37.134220/model-00002-1.84817-0.37582-11.06375-0.28000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.8862 - categorical_accuracy: 0.3366 - val_loss: 11.9422 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1609_52_37.134220/model-00003-1.88621-0.33660-11.94224-0.23000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 2.0313 - categorical_accuracy: 0.2712 - val_loss: 9.7644 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1609_52_37.134220/model-00004-2.03130-0.27124-9.76435-0.23000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.8225 - categorical_accuracy: 0.3529 - val_loss: 11.8573 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1609_52_37.134220/model-00005-1.82246-0.35294-11.85729-0.23000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.7948 - categorical_accuracy: 0.3235 - val_loss: 11.8970 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1609_52_37.134220/model-00006-1.79478-0.32353-11.89698-0.23000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.7195 - categorical_accuracy: 0.3333 - val_loss: 9.2343 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1609_52_37.134220/model-00007-1.71948-0.33333-9.23431-0.24000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.5634 - categorical_accuracy: 0.4020 - val_loss: 9.5996 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1609_52_37.134220/model-00008-1.56339-0.40196-9.59961-0.24000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.5363 - categorical_accuracy: 0.4281 - val_loss: 11.4763 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1609_52_37.134220/model-00009-1.53630-0.42810-11.47627-0.23000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.5233 - categorical_accuracy: 0.4118 - val_loss: 10.3966 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1609_52_37.134220/model-00010-1.52329-0.41176-10.39657-0.23000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.5667 - categorical_accuracy: 0.3987 - val_loss: 7.9563 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1609_52_37.134220/model-00011-1.56673-0.39869-7.95633-0.25000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.3577 - categorical_accuracy: 0.4379 - val_loss: 4.8145 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1609_52_37.134220/model-00012-1.35767-0.43791-4.81451-0.30000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.6759 - categorical_accuracy: 0.3399 - val_loss: 4.3456 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1609_52_37.134220/model-00013-1.67588-0.33987-4.34559-0.36000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.4758 - categorical_accuracy: 0.4052 - val_loss: 2.5255 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1609_52_37.134220/model-00014-1.47575-0.40523-2.52554-0.40000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.4312 - categorical_accuracy: 0.4052 - val_loss: 1.5818 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1609_52_37.134220/model-00015-1.43119-0.40523-1.58180-0.44000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.4968 - categorical_accuracy: 0.3758 - val_loss: 2.1890 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1609_52_37.134220/model-00016-1.49685-0.37582-2.18897-0.40000.h5\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 37s 1s/step - loss: 1.3185 - categorical_accuracy: 0.4837 - val_loss: 2.1230 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1609_52_37.134220/model-00017-1.31849-0.48366-2.12297-0.26000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.4587 - categorical_accuracy: 0.4118 - val_loss: 2.3395 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1609_52_37.134220/model-00018-1.45869-0.41176-2.33950-0.28000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.1801 - categorical_accuracy: 0.5229 - val_loss: 2.2800 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1609_52_37.134220/model-00019-1.18008-0.52288-2.28002-0.31000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 37s 1s/step - loss: 1.1761 - categorical_accuracy: 0.5392 - val_loss: 2.2279 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1609_52_37.134220/model-00020-1.17611-0.53922-2.22794-0.32000.h5\n"
     ]
    }
   ],
   "source": [
    "train_gen = DataGenerator(affine=True, flip=True,edge=True)\n",
    "val_gen = DataGenerator()\n",
    "model_gen = ModelGenerator()\n",
    "\n",
    "input_shape = (30,120,120, 3)\n",
    "num_classes = 5\n",
    "\n",
    "model = model_gen.c3d2(input_shape, num_classes)\n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "\n",
    "train_generator = train_gen.generator(train_path, train_doc, batch_size)\n",
    "val_generator = val_gen.generator(val_path, val_doc, batch_size)\n",
    "train(batch_size, num_epochs, model, train_generator, val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There was no improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color: #339966;\">Back to vanilla code</span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Final Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters initialization\n",
    "nb_rows = 120   # X dimension of the image\n",
    "nb_cols = 120   # Y dimesnion of the image\n",
    "#total_frames = 30\n",
    "nb_frames = 30  # lenght of the video frames\n",
    "nb_channel = 3 # numbe rof channels in images 3 for color(RGB) and 1 for Gray\n",
    "\n",
    "# Helper function to generate a random affine transform on the iamge\n",
    "def get_random_affine():\n",
    "    dx, dy = np.random.randint(-1.7, 1.8, 2)\n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    return M\n",
    "\n",
    "# Helper function to normalise the data\n",
    "def normalize_data(data):\n",
    "    return data/127.5-1\n",
    "\n",
    "# Helper function to initialize all the batch image data and labels\n",
    "def init_batch_data(batch_size):\n",
    "    batch_data = np.zeros((batch_size, nb_frames, nb_rows, nb_cols, nb_channel)) \n",
    "    batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "    return batch_data, batch_labels\n",
    "\n",
    "def load_batch_images(source_path, folder_list, batch_num, batch_size, t,validation):\n",
    "    \n",
    "    batch_data,batch_labels = init_batch_data(batch_size)\n",
    "    \n",
    "    # We will also build an augumented batch data with affine transformation\n",
    "    batch_data_aug,batch_labels_aug = init_batch_data(batch_size)\n",
    "    \n",
    "    # We will also build an augmented batch data with horizontal flip\n",
    "    batch_data_flip,batch_labels_flip = init_batch_data(batch_size)\n",
    "    \n",
    "    #create a list of image numbers you want to use for a particular video using full frames\n",
    "    img_idx = [x for x in range(0, nb_frames)] \n",
    "\n",
    "    for folder in range(batch_size): # iterate over the batch_size\n",
    "        # read all the images in the folder\n",
    "        imgs = sorted(os.listdir(source_path+'/'+ t[folder + (batch_num*batch_size)].split(';')[0])) \n",
    "        # Generate a random affine to be used in image transformation for buidling agumented data set\n",
    "        M = get_random_affine()\n",
    "        \n",
    "        #  Iterate over the frames/images of a folder to read them in\n",
    "        for idx, item in enumerate(img_idx): \n",
    "            ## image = imread(source_path+'/'+ t[folder + (batch_num*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "            image = cv2.imread(source_path+'/'+ t[folder + (batch_num*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Cropping non symmetric frames\n",
    "            if image.shape[0] != image.shape[1]:\n",
    "                image=image[0:120,20:140]\n",
    "            \n",
    "            #crop the images and resize them. Note that the images are of 2 different shape \n",
    "            #and the conv3D will throw error if the inputs in a batch have different shapes   \n",
    "            resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "            #Normal data\n",
    "            batch_data[folder,idx] = (resized)\n",
    "            \n",
    "            #Data with affine transformation\n",
    "            batch_data_aug[folder,idx] = (cv2.warpAffine(resized, M, (resized.shape[0], resized.shape[1])))\n",
    "            \n",
    "            # Data with horizontal flip\n",
    "            batch_data_flip[folder,idx]= np.flip(resized,1)\n",
    "\n",
    "        batch_labels[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "        batch_labels_aug[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "        \n",
    "        # Labeling data with horizobtal flip, right swipe becomes left swipe and viceversa\n",
    "        if int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==0:\n",
    "                    batch_labels_flip[folder, 1] = 1\n",
    "        elif int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==1:\n",
    "                    batch_labels_flip[folder, 0] = 1\n",
    "                    \n",
    "        else:\n",
    "                    batch_labels_flip[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n",
    "                  \n",
    "    \n",
    "    batch_data_final = np.append(batch_data, batch_data_aug, axis = 0)\n",
    "    batch_data_final = np.append(batch_data_final, batch_data_flip, axis = 0)\n",
    "\n",
    "    batch_labels_final = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "    batch_labels_final = np.append(batch_labels_final, batch_labels_flip, axis = 0)\n",
    "    \n",
    "    if validation:\n",
    "        batch_data_final=batch_data\n",
    "        batch_labels_final= batch_labels\n",
    "        \n",
    "    return batch_data_final,batch_labels_final\n",
    "\n",
    "def generator(source_path, folder_list, batch_size, validation=False):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            # you yield the batch_data and the batch_labels, remember what does yield do\n",
    "            yield load_batch_images(source_path, folder_list, batch, batch_size, t,validation)\n",
    "            \n",
    "\n",
    "        \n",
    "        # Code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            yield load_batch_images(source_path, folder_list, batch, batch_size, t,validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = './Project_data/train'\n",
    "val_path = './Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters = [8,16,32,64]\n",
    "nb_dense = [256, 128, 5]\n",
    "\n",
    "# Input\n",
    "input_shape=(nb_frames,nb_rows,nb_cols,nb_channel)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(nb_filters[0], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=input_shape,\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[1], \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[2], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[3], \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(nb_dense[0], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_dense[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(nb_dense[2], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 864,101\n",
      "Trainable params: 863,989\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = Adam() #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size,validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# write the Reducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 10\n",
      "Source path =  ./Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n",
      "67/67 [==============================] - 95s 1s/step - loss: 1.7902 - categorical_accuracy: 0.2622 - val_loss: 1.4901 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1610_47_14.696608/model-00001-1.79432-0.26144-1.49009-0.37000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 29s 434ms/step - loss: 1.5055 - categorical_accuracy: 0.3847 - val_loss: 1.4563 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1610_47_14.696608/model-00002-1.50551-0.38474-1.45627-0.32000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 30s 452ms/step - loss: 1.4249 - categorical_accuracy: 0.4312 - val_loss: 1.4526 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1610_47_14.696608/model-00003-1.42493-0.43118-1.45265-0.29000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 31s 469ms/step - loss: 1.2961 - categorical_accuracy: 0.4726 - val_loss: 1.3099 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1610_47_14.696608/model-00004-1.29612-0.47264-1.30985-0.43000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 31s 458ms/step - loss: 1.4165 - categorical_accuracy: 0.4113 - val_loss: 1.2360 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1610_47_14.696608/model-00005-1.41648-0.41128-1.23605-0.51000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 31s 459ms/step - loss: 1.1353 - categorical_accuracy: 0.5755 - val_loss: 1.6127 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1610_47_14.696608/model-00006-1.13529-0.57546-1.61266-0.28000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 30s 452ms/step - loss: 1.0561 - categorical_accuracy: 0.5605 - val_loss: 2.6672 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1610_47_14.696608/model-00007-1.05605-0.56053-2.66717-0.35000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 30s 453ms/step - loss: 1.0106 - categorical_accuracy: 0.6385 - val_loss: 1.0483 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1610_47_14.696608/model-00008-1.01058-0.63847-1.04828-0.58000.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 31s 467ms/step - loss: 0.8111 - categorical_accuracy: 0.6816 - val_loss: 0.9084 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1610_47_14.696608/model-00009-0.81108-0.68159-0.90836-0.67000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 30s 450ms/step - loss: 0.8425 - categorical_accuracy: 0.6766 - val_loss: 0.7758 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1610_47_14.696608/model-00010-0.84251-0.67662-0.77578-0.69000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 30s 444ms/step - loss: 0.7715 - categorical_accuracy: 0.7048 - val_loss: 0.7624 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1610_47_14.696608/model-00011-0.77147-0.70481-0.76243-0.76000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 29s 436ms/step - loss: 0.5056 - categorical_accuracy: 0.7910 - val_loss: 0.8075 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1610_47_14.696608/model-00012-0.50561-0.79104-0.80747-0.70000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 31s 457ms/step - loss: 0.7224 - categorical_accuracy: 0.7330 - val_loss: 0.7832 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1610_47_14.696608/model-00013-0.72240-0.73300-0.78324-0.69000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 31s 462ms/step - loss: 0.5918 - categorical_accuracy: 0.7645 - val_loss: 0.6094 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1610_47_14.696608/model-00014-0.59181-0.76451-0.60940-0.79000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 30s 446ms/step - loss: 0.5236 - categorical_accuracy: 0.7944 - val_loss: 0.6594 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1610_47_14.696608/model-00015-0.52362-0.79436-0.65941-0.74000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 31s 461ms/step - loss: 0.4970 - categorical_accuracy: 0.7960 - val_loss: 0.5551 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1610_47_14.696608/model-00016-0.49697-0.79602-0.55510-0.78000.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 31s 456ms/step - loss: 0.4622 - categorical_accuracy: 0.7993 - val_loss: 0.5611 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1610_47_14.696608/model-00017-0.46216-0.79934-0.56113-0.76000.h5\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 31s 456ms/step - loss: 0.4240 - categorical_accuracy: 0.8375 - val_loss: 0.5493 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1610_47_14.696608/model-00018-0.42397-0.83748-0.54930-0.77000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 30s 455ms/step - loss: 0.3837 - categorical_accuracy: 0.8474 - val_loss: 0.6224 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1610_47_14.696608/model-00019-0.38370-0.84743-0.62236-0.77000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 30s 450ms/step - loss: 0.4329 - categorical_accuracy: 0.8358 - val_loss: 0.5348 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1610_47_14.696608/model-00020-0.43291-0.83582-0.53483-0.80000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96e15a4710>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "num_epochs = 20\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Final Model Accuracy is <span style=\"color: #339966;\">82%</span></h3>\n",
    "<h3>Afer increasing batch size we got <span style=\"color: #339966;\">84%</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/34 [==============================] - 17s 503ms/step - loss: 0.4356 - categorical_accuracy: 0.8431 - val_loss: 0.3185 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2019-03-1610_47_14.696608/model-00001-0.43559-0.84314-0.31854-0.86000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 15s 446ms/step - loss: 0.4096 - categorical_accuracy: 0.8431 - val_loss: 0.5677 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00002: saving model to model_init_2019-03-1610_47_14.696608/model-00002-0.40964-0.84314-0.56767-0.78000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 15s 447ms/step - loss: 0.4417 - categorical_accuracy: 0.8595 - val_loss: 0.3868 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00003: saving model to model_init_2019-03-1610_47_14.696608/model-00003-0.44172-0.85948-0.38684-0.82000.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 16s 457ms/step - loss: 0.2922 - categorical_accuracy: 0.8791 - val_loss: 0.5231 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00004: saving model to model_init_2019-03-1610_47_14.696608/model-00004-0.29219-0.87908-0.52306-0.78000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 15s 451ms/step - loss: 0.2572 - categorical_accuracy: 0.9183 - val_loss: 0.5081 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2019-03-1610_47_14.696608/model-00005-0.25718-0.91830-0.50805-0.80000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 16s 467ms/step - loss: 0.2394 - categorical_accuracy: 0.9150 - val_loss: 0.3640 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00006: saving model to model_init_2019-03-1610_47_14.696608/model-00006-0.23940-0.91503-0.36400-0.80000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 16s 463ms/step - loss: 0.2213 - categorical_accuracy: 0.9150 - val_loss: 0.4640 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00007: saving model to model_init_2019-03-1610_47_14.696608/model-00007-0.22125-0.91503-0.46398-0.82000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 15s 442ms/step - loss: 0.2946 - categorical_accuracy: 0.8856 - val_loss: 0.4519 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00008: saving model to model_init_2019-03-1610_47_14.696608/model-00008-0.29457-0.88562-0.45189-0.86000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 15s 446ms/step - loss: 0.3238 - categorical_accuracy: 0.8824 - val_loss: 0.2618 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00009: saving model to model_init_2019-03-1610_47_14.696608/model-00009-0.32375-0.88235-0.26175-0.86000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 15s 454ms/step - loss: 0.2708 - categorical_accuracy: 0.9118 - val_loss: 0.5215 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00010: saving model to model_init_2019-03-1610_47_14.696608/model-00010-0.27078-0.91176-0.52152-0.74000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 16s 459ms/step - loss: 0.3329 - categorical_accuracy: 0.8660 - val_loss: 0.5126 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00011: saving model to model_init_2019-03-1610_47_14.696608/model-00011-0.33292-0.86601-0.51261-0.82000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 16s 481ms/step - loss: 0.2503 - categorical_accuracy: 0.9150 - val_loss: 0.2941 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00012: saving model to model_init_2019-03-1610_47_14.696608/model-00012-0.25030-0.91503-0.29411-0.90000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 15s 455ms/step - loss: 0.1782 - categorical_accuracy: 0.9412 - val_loss: 0.5441 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00013: saving model to model_init_2019-03-1610_47_14.696608/model-00013-0.17818-0.94118-0.54406-0.78000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 16s 472ms/step - loss: 0.2577 - categorical_accuracy: 0.8987 - val_loss: 0.3327 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00014: saving model to model_init_2019-03-1610_47_14.696608/model-00014-0.25775-0.89869-0.33270-0.80000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 15s 448ms/step - loss: 0.2406 - categorical_accuracy: 0.9183 - val_loss: 0.3130 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00015: saving model to model_init_2019-03-1610_47_14.696608/model-00015-0.24057-0.91830-0.31304-0.86000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 16s 471ms/step - loss: 0.2357 - categorical_accuracy: 0.9248 - val_loss: 0.5454 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00016: saving model to model_init_2019-03-1610_47_14.696608/model-00016-0.23574-0.92484-0.54538-0.78000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 15s 455ms/step - loss: 0.3551 - categorical_accuracy: 0.8889 - val_loss: 0.5084 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00017: saving model to model_init_2019-03-1610_47_14.696608/model-00017-0.35505-0.88889-0.50843-0.80000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 16s 458ms/step - loss: 0.2511 - categorical_accuracy: 0.9346 - val_loss: 0.3294 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00018: saving model to model_init_2019-03-1610_47_14.696608/model-00018-0.25112-0.93464-0.32935-0.88000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 16s 461ms/step - loss: 0.2835 - categorical_accuracy: 0.9085 - val_loss: 0.4371 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00019: saving model to model_init_2019-03-1610_47_14.696608/model-00019-0.28353-0.90850-0.43712-0.82000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 16s 468ms/step - loss: 0.2946 - categorical_accuracy: 0.8791 - val_loss: 0.3113 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00020: saving model to model_init_2019-03-1610_47_14.696608/model-00020-0.29462-0.87908-0.31134-0.84000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f979c8839b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
